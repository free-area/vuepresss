---

headerDepth: 2
sidebar: heading
---



# 定义

****{#def1}

首先给出NODE的定义：
设有如下高阶常微分方程和初始条件：

::: info 定义 
$$
\begin{equation}	
		\label{eq:ode}
        \tag{1}
		\frac{dh}{dt}(t)=f_{\theta}(t, h(t)) ,\quad h(0)=x
	\end{equation}
$$
其中$f_{\theta}: \mathbb{R} \times \mathbb{R}^{d}\rightarrow R^d$是神经网络架构（比如全连接神经网络），称该微分方程为神经常微分方程，简称NeuralODE。在$[0,T]$上关于$h$ 是$Lipschitz$连续的，$\theta$ 是该网络架构的参数，根据picard存在定理，
	初值问题$ivp[f_{\theta},x]$有唯一解$\varphi(x,t)$，当$t=T$固定时,函数$\varphi(x,T):R^d \rightarrow R^d$以$x$作为特征输入可以看成一个神经网络，$T$称为终止时间,表示网络的深度。	

:::



回顾下一个resnet网络架构中的resnet块：
$$
\begin{equation}\label{resnet}
	\tag{2}
	h_{j+1}=h_j+f_{\theta_j} \left(j, h_j\right) \text {, }\\
	h_0=x,j=0, \ldots, T-1.
\end{equation}
$$
函数$f_{\theta_j}$是带参数的神经网络函数，$T$是网络层数。
将区间分割为$\Delta t$为单位,通过使用euler法来可以求得$\ref{eq:ode}$的每个时刻$h_t$，
那么就有
$$
\begin{equation}\label{node_discret}
\tag{3}
	h\left(t_{j+1}\right)\approx h\left(t_j\right)+\Delta t f_{\theta}\left(t_j, h\left(t_j\right)\right) .
\end{equation}
$$
观察$\ref{resnet}$和$\ref{node_discret}$可以发现每个resnet块类似于欧拉法求数值解微分方程时步长为1的情形（$\Delta t=1$）。

在常规神经网络中，我们从输出$x$出发得到隐状态层$h_1,h_2,\cdots ,h_n$(假设是$n$层神经网络)， 每层$h_m$参数化，参数量随着层数的增加而增加。
所以若我们在renet层级间加入更多的层，且最终趋向于添加了无穷层时，残差神经网络架构就与欧拉法解方程统一起来：resnet前向传播可以看做时欧拉法解神经微分方程，而神经微分方程就是参数共享的resnet网络模型的连续化。
后面还会从rnn的角度和ode进行统一建模，其实目前大部分有效的流行的深度学习框架都类似于微分方程，若干个世纪以来，微分方程一直被用来做应用建模。

目前可以看出研究和使用node的一个好处在于ode的理论优势，数学领域，对ode的研究不管是理论还是数值分析，建模方法，训练思路都是丰富且先进，可以帮助我们更好的进行深度学习的研究。
由于ode的求解比较难，虽然可以使用ode求解器，但是由于维度巨大（例如一个1080p像素点有$1920\times1080$个），计算的时间复杂仍然很高，一般将node作为深度学习网络框架中的一部分，node的输入是被前神经网络处理过的维度较低的数据，同时对node的输出再进行处理，例如添加一层全连接层或者非线性层，这也大幅提高NeuralODE的表达能力。
下面给出更一般形式的含有node的网络架构：
$$
\begin{equation}
\label{node_gene1}
\tag{4}
	\left\{\begin{array}{l}
		\frac{dh}{dt}=f_{\theta_t}(s, \mathrm{x}, \mathrm{h}(t)) \\
		h(0)=f_{in}(\mathrm{x}) \\
		out=f_{out}(h(T))
	\end{array} \right. 
\end{equation}
$$




::: tip 	

虽然NODE中的输入和输出是固定的维度，但是网络架构$f_{\theta(t)}(s, \mathrm{x}, \mathrm{h}(t))$是任意的，可以出现更高维度。在后续会深入讨论网络架构的选择和设计。
:::



