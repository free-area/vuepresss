import{_ as r}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as s,o as d,c as _,b as e,e as t,w as l,d as o}from"./app-ZXNuZZLu.js";const i={},c=e("p",null,[o("神经常微分方程（简称：NeuralODE）是一种通过使用神经网络参数化向量场来结合常微分方程和神经网络的一种全新的深度学习模型。在2018年首次被提出和使用"),e("sup",{class:"footnote-ref"},[e("a",{href:"#footnote1"},"[1]"),e("a",{class:"footnote-anchor",id:"footnote-ref1"})]),o("，它将传统的有限层神经网络架构转变为参数共享的无限层神经网络架构，弥合了深度学习和动态系统的差距。NODE从网络架构，训练方法，超参数选择，表示能力等均和传统的神经网络例如CNN,MLP，RNN等有所不同。")],-1),u=e("p",null,"本章首先给出神经常微分方程(以后简称NODE)的定义，NODE模型的思想来源。然后介绍针对NODE的一种全新的反向传播方法-adjoint sensitivity 方法和相应的更广义的证明，对于NODE的表示能力用严格的数学理论进行详细论述，同时对训练用到的超参数的选择，网络架构的选取进行讨论。",-1),f=e("hr",{class:"footnotes-sep"},null,-1),E={class:"footnotes"},h={class:"footnotes-list"},m={id:"footnote1",class:"footnote-item"},p={href:"https://arxiv.org/abs/1806.07366",target:"_blank",rel:"noopener noreferrer"},B=e("a",{href:"#footnote-ref1",class:"footnote-backref"},"↩︎",-1);function N(x,A){const n=s("RouterLink"),a=s("ExternalLinkIcon");return d(),_("div",null,[c,u,e("ul",null,[e("li",null,[e("p",null,[t(n,{to:"/dl/neural_de/node/%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86.html"},{default:l(()=>[o("预备知识.md")]),_:1})])]),e("li",null,[e("p",null,[t(n,{to:"/dl/neural_de/node/%E5%AE%9A%E4%B9%89.html"},{default:l(()=>[o("定义.md")]),_:1})])]),e("li",null,[e("p",null,[t(n,{to:"/dl/neural_de/node/%E8%A1%A8%E7%A4%BA%E5%92%8C%E9%80%BC%E8%BF%91.html"},{default:l(()=>[o("表示和逼近能力.md")]),_:1})])]),e("li",null,[e("p",null,[t(n,{to:"/dl/neural_de/node/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.html"},{default:l(()=>[o("优化算法.md")]),_:1})])]),e("li",null,[e("p",null,[t(n,{to:"/dl/neural_de/node/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.html"},{default:l(()=>[o("设计网络架构.md")]),_:1})])])]),f,e("section",E,[e("ol",h,[e("li",m,[e("p",null,[o("["),e("a",p,[o("1806.07366] Neural Ordinary Differential Equations (arxiv.org)"),t(a)]),o(),B])])])])])}const D=r(i,[["render",N],["__file","index.html.vue"]]);export{D as default};
